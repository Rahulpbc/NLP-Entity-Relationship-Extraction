{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LINGUISTICMACHINES_Demo.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO2Pby+Tr0D/+6d9chesrtC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahulpbc/NLP-Entity-Relationship-Extraction/blob/main/LINGUISTICMACHINES_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxurybkz29a8",
        "outputId": "d4a54b8c-f226-40e8-c901-9ee4f2bbc0db"
      },
      "source": [
        "import spacy\n",
        "from nltk.corpus import wordnet as wn\n",
        "from spacy import displacy\n",
        "from IPython.core.display import display, HTML\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "TokenNLP = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "SentenceNLP = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"ner\"])\n",
        "SentenceNLP.pipeline\n",
        "\n",
        "outputfile=open(\"outputfile.txt\",\"w\")\n"
      ],
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "PHxJc8Pp8jJ5",
        "outputId": "3a2590a6-1307-4842-bdf0-b4a099ad0a7d"
      },
      "source": [
        "# Task 1- To read the dataset from the local system\n",
        "from google.colab import files\n",
        "uploaded=files.upload()\n",
        "filename=None\n",
        "for name, data in uploaded.items():\n",
        "  filename=name\n",
        "\n",
        "print(filename)"
      ],
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-af6331b5-c08d-4fe4-8392-4c98768fb217\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-af6331b5-c08d-4fe4-8392-4c98768fb217\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test_sentence.txt to test_sentence.txt\n",
            "test_sentence.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71cvoqcw8qFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ce2fb7-31e5-4985-9f39-9ab04b864b80"
      },
      "source": [
        "# Task 1- The dataset can also be read directly from collab\n",
        "name=input(\"enter file name if you uploaded directly from the left\")\n",
        "filename=name+\".txt\""
      ],
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "enter file name if you uploaded directly from the leftsemeval_test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQB_uwwe9G6N",
        "outputId": "3e05f783-c2db-432a-e508-76c0b597b353"
      },
      "source": [
        "# splitting the dataset into sentences\n",
        "with open(filename,encoding='utf-8-sig') as fd:\n",
        "  lines = fd.read().split(\"\\n\\n\")\n",
        "  #lines = fd.read().splitlines()\n",
        "  while '' in lines:\n",
        "    lines.remove('')\n",
        "print(\"The Document is split by \\\\n\\\\n and appends each sentence to the list:\")\n",
        "print(lines[0])"
      ],
      "execution_count": 322,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Document is split by \\n\\n and appends each sentence to the list:\n",
            "8007\t\"A child is told a <e1>lie</e1> for several years by their <e2>parents</e2> before he/she realizes that a Santa Claus does not exist.\"\n",
            "Product-Producer(e1,e2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnmbWz7_3-on"
      },
      "source": [
        "# Pre-processing the sentence to remove annotations and extract the entities and relationship\n",
        "import re\n",
        "\n",
        "sentences = []\n",
        "entity1 = []\n",
        "entity2 = []\n",
        "rel = []\n",
        "direction = []\n",
        "in_between_list = []\n",
        "count = 0\n",
        "\n",
        "for l in lines:\n",
        "  # Extracting entity 1\n",
        "  entity = re.findall(r'\\s*<e1>([^<]*)</e1>', l)\n",
        "  \n",
        "  if not entity:\n",
        "    entity1.append(\"\")\n",
        "  else:\n",
        "    entity1.append(entity[0])\n",
        "\n",
        "  # Extracting entity 2\n",
        "  entity = re.findall(r'\\s*<e2>([^<]*)\\</e2>', l)\n",
        "  \n",
        "  if not entity:\n",
        "    entity2.append(\"\")\n",
        "  else:\n",
        "    entity2.append(entity[0])\n",
        "  \n",
        "  # Extracting the words in between the entities\n",
        "  in_bet_words = re.findall(r'</e1>([^<]*)\\<e2>', l)\n",
        "  in_between_list.append(in_bet_words[0])\n",
        " \n",
        "  #Extracting the relation\n",
        "  relation = re.split('\"\\\\n',l)[1]\n",
        "  \n",
        "  # Extracting the direction\n",
        "  dir = relation[relation.find(\"(\")+1:relation.find(\")\")]\n",
        "\n",
        "  # Pre-processing to remove the annotations \n",
        "  l = l.split('\"',1)[1].lstrip()\n",
        "  l = l.rsplit('\"',1)[0]\n",
        "  l = l.replace('<e1>','')\n",
        "  l = l.replace('<e2>','')\n",
        "  l = l.replace('</e1>','')\n",
        "  l = l.replace('</e2>','')\n",
        "  sentences.append(l)\n",
        "  rel.append(relation)\n",
        "  direction.append(dir)\n"
      ],
      "execution_count": 323,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_PH6vmo4R79"
      },
      "source": [
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()"
      ],
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KUVJYWEH4Swo",
        "outputId": "15d59130-f422-4efd-e968-84b4d27a91a5"
      },
      "source": [
        "# Task 2 of the project\n",
        "\n",
        "outputfile=open(\"outputfile2.txt\",\"w\")\n",
        "for sent in sentences:\n",
        "    tokendoc = TokenNLP(sent)\n",
        "    tokenlist=[]\n",
        "    lemmalist=[]\n",
        "    poslist=[]\n",
        "    taglist=[]\n",
        "    depedenlist=[]\n",
        "    for token in tokendoc:\n",
        "      tokenlist.append(token.text)\n",
        "      lemmalist.append(token.lemma_)\n",
        "      poslist.append(token.pos_)\n",
        "      taglist.append(token.tag_)\n",
        "      depedenlist.append(token.dep_)\n",
        "    \n",
        "    print(\"Sentence:\")\n",
        "    outputfile.writelines(\"Sentence:\\n\")\n",
        "    print(sent)\n",
        "    print(sent)\n",
        "    outputfile.writelines(sent+\"\\n\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\"the tokens of the sentence are:\")\n",
        "    outputfile.writelines(\"the tokens of the sentence are:\\n\")\n",
        "    print(tokenlist)\n",
        "    outputfile.writelines(str(tokenlist)+\"\\n\")\n",
        "\n",
        "    print(\"the lemmas of the sentence are\")\n",
        "    outputfile.writelines(\"the lemmas of the sentence are:\\n\")\n",
        "    print(lemmalist)\n",
        "    outputfile.writelines(str(lemmalist)+\"\\n\")\n",
        "\n",
        "    print(\"the POS TAGS of the sentence are\")\n",
        "    outputfile.writelines(\"the POS TAGS of the sentence are:\\n\")\n",
        "    print(poslist)\n",
        "    outputfile.writelines(str(poslist)+\"\\n\")\n",
        "\n",
        "    print(\"the DEP TAGS of the sentence are\")\n",
        "    outputfile.writelines(\"the DEP TAGS of the sentence are:\\n\")\n",
        "    print(depedenlist)\n",
        "    outputfile.writelines(str(depedenlist)+\"\\n\")\n",
        "    html = displacy.render(tokendoc, style=\"dep\")\n",
        "\n",
        "    print(\"the NER Tags of the sentence are\")\n",
        "    doc = nlp(sent)\n",
        "    outputfile.writelines(\"The NER Tags of the sentence are:\\n\")\n",
        "    outputfile.writelines(str([(X.text, X.label_) for X in doc.ents])+\"\\n\")\n",
        "    print([(X.text, X.label_) for X in doc.ents])\n",
        "    print(\"-------------TOKEN DETAILS----------------\")\n",
        "    outputfile.writelines(\"-------------TOKEN DETAILS----------------\\n\")\n",
        "    for token in tokendoc:\n",
        "      print(\"token:\")\n",
        "      outputfile.writelines(\"tokens:\\n\")\n",
        "      print(token.text+\" and lemma is \"+ str(token.lemma_))\n",
        "      outputfile.writelines(token.text+\"  and lemma is \"+ str(token.lemma_)+\"\\n\")\n",
        "      print(\"-------\")\n",
        "      outputfile.writelines(\"-------\\n\")\n",
        "      for ss in wn.synsets(token.lemma_):\n",
        "        print(\"SYSNSET:\"+str(ss))\n",
        "        outputfile.writelines(\"SYSNSET:\"+str(ss)+\"\\n\")\n",
        "        print (\"hypernyms: \"+str(ss.hypernyms())+\" hyponyms: \" + str(ss.hyponyms())+\" holonyms: \" + str(ss.member_holonyms())+\" meronyms: \"+ str(ss.part_meronyms()))\n",
        "        outputfile.writelines(\"hypernyms: \"+str(ss.hypernyms())+\" hyponyms: \"+str(ss.hyponyms())+\" holonyms: \"+str(ss.member_holonyms())+\" meronyms: \"+str(ss.part_meronyms())+\"\\n\")\n",
        "      print(\"-----------------------------------\")\n",
        "      outputfile.writelines(\"-----------------------------------\\n\")\n",
        "\n",
        "\n",
        "    display(HTML(html))\n",
        "    \n",
        "    print(\"***************************************************************\")\n",
        "    outputfile.writelines(\"***************************************************************\\n\")\n",
        "\n",
        "\n",
        "outputfile.close()"
      ],
      "execution_count": 325,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence:\n",
            "A child is told a lie for several years by their parents before he/she realizes that a Santa Claus does not exist.\n",
            "A child is told a lie for several years by their parents before he/she realizes that a Santa Claus does not exist.\n",
            "\n",
            "\n",
            "the tokens of the sentence are:\n",
            "['A', 'child', 'is', 'told', 'a', 'lie', 'for', 'several', 'years', 'by', 'their', 'parents', 'before', 'he', '/', 'she', 'realizes', 'that', 'a', 'Santa', 'Claus', 'does', 'not', 'exist', '.']\n",
            "the lemmas of the sentence are\n",
            "['a', 'child', 'be', 'tell', 'a', 'lie', 'for', 'several', 'year', 'by', '-PRON-', 'parent', 'before', '-PRON-', '/', '-PRON-', 'realize', 'that', 'a', 'Santa', 'Claus', 'do', 'not', 'exist', '.']\n",
            "the POS TAGS of the sentence are\n",
            "['DET', 'NOUN', 'AUX', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PRON', 'CCONJ', 'PRON', 'VERB', 'SCONJ', 'DET', 'PROPN', 'PROPN', 'AUX', 'PART', 'VERB', 'PUNCT']\n",
            "the DEP TAGS of the sentence are\n",
            "['det', 'nsubjpass', 'auxpass', 'ROOT', 'det', 'dobj', 'prep', 'amod', 'pobj', 'agent', 'poss', 'pobj', 'prep', 'pobj', 'punct', 'nsubj', 'ROOT', 'mark', 'det', 'compound', 'nsubj', 'aux', 'neg', 'ccomp', 'punct']\n",
            "the NER Tags of the sentence are\n",
            "[('several years', 'DATE'), ('Santa Claus', 'GPE')]\n",
            "-------------TOKEN DETAILS----------------\n",
            "token:\n",
            "A and lemma is a\n",
            "-------\n",
            "SYSNSET:Synset('angstrom.n.01')\n",
            "hypernyms: [Synset('metric_linear_unit.n.01')] hyponyms: [] holonyms: [] meronyms: [Synset('picometer.n.01')]\n",
            "SYSNSET:Synset('vitamin_a.n.01')\n",
            "hypernyms: [Synset('fat-soluble_vitamin.n.01')] hyponyms: [Synset('vitamin_a1.n.01'), Synset('vitamin_a2.n.01')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('deoxyadenosine_monophosphate.n.01')\n",
            "hypernyms: [Synset('nucleotide.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('adenine.n.01')\n",
            "hypernyms: [Synset('purine.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('ampere.n.02')\n",
            "hypernyms: [Synset('current_unit.n.01')] hyponyms: [] holonyms: [] meronyms: [Synset('milliampere.n.01')]\n",
            "SYSNSET:Synset('a.n.06')\n",
            "hypernyms: [Synset('letter.n.02')] hyponyms: [] holonyms: [Synset('roman_alphabet.n.01')] meronyms: []\n",
            "SYSNSET:Synset('a.n.07')\n",
            "hypernyms: [Synset('blood_group.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "-----------------------------------\n",
            "token:\n",
            "child and lemma is child\n",
            "-------\n",
            "SYSNSET:Synset('child.n.01')\n",
            "hypernyms: [Synset('juvenile.n.01')] hyponyms: [Synset('bairn.n.01'), Synset('buster.n.02'), Synset('changeling.n.02'), Synset('child_prodigy.n.01'), Synset('foster-child.n.01'), Synset('imp.n.02'), Synset('kiddy.n.01'), Synset('orphan.n.01'), Synset('peanut.n.03'), Synset('pickaninny.n.01'), Synset('poster_child.n.01'), Synset('preschooler.n.01'), Synset('silly.n.01'), Synset('sprog.n.02'), Synset('toddler.n.01'), Synset('urchin.n.01'), Synset('waif.n.01')] holonyms: [] meronyms: [Synset('child's_body.n.01')]\n",
            "SYSNSET:Synset('child.n.02')\n",
            "hypernyms: [Synset('offspring.n.01')] hyponyms: [Synset('army_brat.n.01'), Synset('baby.n.01'), Synset('female_offspring.n.01'), Synset('male_offspring.n.01'), Synset('stepchild.n.01')] holonyms: [Synset('family.n.02')] meronyms: []\n",
            "SYSNSET:Synset('child.n.03')\n",
            "hypernyms: [Synset('person.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('child.n.04')\n",
            "hypernyms: [Synset('descendant.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "-----------------------------------\n",
            "token:\n",
            "is and lemma is be\n",
            "-------\n",
            "SYSNSET:Synset('beryllium.n.01')\n",
            "hypernyms: [Synset('metallic_element.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('be.v.01')\n",
            "hypernyms: [] hyponyms: [Synset('abound.v.01'), Synset('accept.v.08'), Synset('account.v.01'), Synset('account_for.v.01'), Synset('act.v.06'), Synset('answer.v.06'), Synset('appear.v.04'), Synset('bake.v.04'), Synset('balance.v.04'), Synset('be_well.v.01'), Synset('beat.v.12'), Synset('begin.v.06'), Synset('begin.v.07'), Synset('belong.v.01'), Synset('belong.v.02'), Synset('belong.v.04'), Synset('belong.v.05'), Synset('breathe.v.04'), Synset('buy.v.03'), Synset('clean.v.05'), Synset('cohere.v.03'), Synset('come_in_for.v.01'), Synset('come_in_handy.v.01'), Synset('compact.v.01'), Synset('compare.v.02'), Synset('confuse.v.02'), Synset('connect.v.07'), Synset('consist.v.02'), Synset('consist.v.04'), Synset('contain.v.04'), Synset('contain.v.05'), Synset('continue.v.10'), Synset('cost.v.01'), Synset('count.v.02'), Synset('count.v.07'), Synset('cover.v.18'), Synset('cut.v.25'), Synset('cut_across.v.02'), Synset('deck.v.01'), Synset('depend.v.01'), Synset('deserve.v.01'), Synset('disagree.v.02'), Synset('distribute.v.09'), Synset('diverge.v.02'), Synset('draw.v.21'), Synset('end.v.03'), Synset('fall.v.04'), Synset('fall.v.16'), Synset('feel.v.04'), Synset('figure.v.02'), Synset('fit.v.07'), Synset('gape.v.02'), Synset('go.v.10'), Synset('gravitate.v.02'), Synset('hail.v.02'), Synset('hang.v.06'), Synset('head.v.04'), Synset('hold.v.17'), Synset('hoodoo.v.01'), Synset('hum.v.02'), Synset('impend.v.01'), Synset('incarnate.v.02'), Synset('iridesce.v.01'), Synset('jumble.v.01'), Synset('kill.v.04'), Synset('lend.v.03'), Synset('let_go.v.02'), Synset('lie.v.04'), Synset('litter.v.01'), Synset('loiter.v.01'), Synset('look.v.02'), Synset('look.v.03'), Synset('lubricate.v.01'), Synset('make.v.31'), Synset('make_sense.v.01'), Synset('measure.v.03'), Synset('mope.v.02'), Synset('object.v.02'), Synset('osculate.v.01'), Synset('owe.v.03'), Synset('pay.v.07'), Synset('point.v.10'), Synset('press.v.08'), Synset('promise.v.04'), Synset('prove.v.01'), Synset('put_out.v.06'), Synset('rage.v.02'), Synset('range.v.01'), Synset('rank.v.01'), Synset('rate.v.02'), Synset('recognize.v.08'), Synset('relate.v.04'), Synset('remain.v.03'), Synset('represent.v.03'), Synset('rest.v.01'), Synset('retard.v.02'), Synset('run.v.05'), Synset('run_into.v.01'), Synset('rut.v.01'), Synset('seem.v.03'), Synset('seethe.v.02'), Synset('sell.v.02'), Synset('sell.v.06'), Synset('sell.v.07'), Synset('shine.v.04'), Synset('shine.v.05'), Synset('sparkle.v.02'), Synset('specify.v.03'), Synset('squat.v.02'), Synset('stagnate.v.01'), Synset('stagnate.v.03'), Synset('stand.v.02'), Synset('stand.v.03'), Synset('stand_by.v.03'), Synset('stay.v.01'), Synset('stay.v.04'), Synset('stick.v.04'), Synset('stink.v.01'), Synset('subtend.v.01'), Synset('suck.v.04'), Synset('suffer.v.06'), Synset('suffer.v.08'), Synset('suit.v.02'), Synset('swim.v.03'), Synset('swim.v.04'), Synset('swing.v.10'), Synset('tend.v.01'), Synset('test.v.04'), Synset('total.v.01'), Synset('translate.v.07'), Synset('transplant.v.02'), Synset('trim.v.05'), Synset('underlie.v.01'), Synset('want.v.02'), Synset('wash.v.05'), Synset('wind.v.02'), Synset('work.v.14')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('be.v.02')\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('be.v.03')\n",
            "hypernyms: [] hyponyms: [Synset('attend.v.01'), Synset('belong.v.03'), Synset('center_on.v.02'), Synset('come.v.10'), Synset('cover.v.03'), Synset('extend.v.06'), Synset('face.v.04'), Synset('follow.v.08'), Synset('go.v.25'), Synset('inhabit.v.02'), Synset('lie.v.01'), Synset('lie.v.06'), Synset('occupy.v.03'), Synset('populate.v.01'), Synset('reach.v.06'), Synset('run.v.03'), Synset('sit.v.02'), Synset('sit.v.07'), Synset('stand_back.v.01'), Synset('straddle.v.01'), Synset('stretch.v.01')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('exist.v.01')\n",
            "hypernyms: [] hyponyms: [Synset('coexist.v.02'), Synset('come.v.06'), Synset('distribute.v.08'), Synset('dwell.v.02'), Synset('dwell.v.04'), Synset('endanger.v.01'), Synset('flow.v.04'), Synset('indwell.v.01'), Synset('kick_around.v.01'), Synset('preexist.v.01'), Synset('prevail.v.02')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('be.v.05')\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('equal.v.01')\n",
            "hypernyms: [] hyponyms: [Synset('equate.v.02'), Synset('match.v.01'), Synset('represent.v.01'), Synset('translate.v.06')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('constitute.v.01')\n",
            "hypernyms: [] hyponyms: [Synset('compose.v.01'), Synset('fall_into.v.01'), Synset('form.v.02'), Synset('make.v.34'), Synset('present.v.05'), Synset('range.v.04'), Synset('supplement.v.02')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('be.v.08')\n",
            "hypernyms: [] hyponyms: [Synset('cox.v.01'), Synset('vet.v.01')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('embody.v.02')\n",
            "hypernyms: [Synset('typify.v.02')] hyponyms: [Synset('body.v.01'), Synset('exemplify.v.01')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('be.v.10')\n",
            "hypernyms: [Synset('take.v.02')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('be.v.11')\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('be.v.12')\n",
            "hypernyms: [Synset('stay.v.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('cost.v.01')\n",
            "hypernyms: [Synset('be.v.01')] hyponyms: [Synset('set_back.v.03')] holonyms: [] meronyms: []\n",
            "-----------------------------------\n",
            "token:\n",
            "told and lemma is tell\n",
            "-------\n",
            "SYSNSET:Synset('tell.n.01')\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('state.v.01')\n",
            "hypernyms: [Synset('express.v.02')] hyponyms: [Synset('add.v.02'), Synset('announce.v.02'), Synset('answer.v.01'), Synset('articulate.v.05'), Synset('declare.v.01'), Synset('declare.v.07'), Synset('explain.v.02'), Synset('get_out.v.04'), Synset('give.v.04'), Synset('misstate.v.01'), Synset('note.v.01'), Synset('precede.v.05'), Synset('present.v.02'), Synset('summarize.v.02')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('tell.v.02')\n",
            "hypernyms: [Synset('inform.v.01')] hyponyms: [Synset('announce.v.04'), Synset('bespeak.v.01'), Synset('digress.v.01'), Synset('impart.v.01'), Synset('propagandize.v.01'), Synset('publicize.v.01'), Synset('repeat.v.01'), Synset('spill.v.05'), Synset('unwrap.v.02')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('tell.v.03')\n",
            "hypernyms: [Synset('inform.v.01')] hyponyms: [Synset('crack.v.10'), Synset('relate.v.03'), Synset('rhapsodize.v.01'), Synset('yarn.v.01')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('order.v.01')\n",
            "hypernyms: [Synset('request.v.02')] hyponyms: [Synset('call.v.05'), Synset('command.v.02'), Synset('direct.v.01'), Synset('instruct.v.02'), Synset('warn.v.03')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('tell.v.05')\n",
            "hypernyms: [Synset('guess.v.04')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('assure.v.02')\n",
            "hypernyms: [Synset('affirm.v.02')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('tell.v.07')\n",
            "hypernyms: [Synset('inform.v.03')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('distinguish.v.01')\n",
            "hypernyms: [Synset('identify.v.01')] hyponyms: [Synset('contradistinguish.v.01'), Synset('contrast.v.01'), Synset('decouple.v.02'), Synset('demarcate.v.01'), Synset('discriminate.v.01'), Synset('discriminate.v.02'), Synset('individualize.v.01'), Synset('know.v.10'), Synset('label.v.04'), Synset('label.v.05'), Synset('severalize.v.01'), Synset('sex.v.02'), Synset('stratify.v.01')] holonyms: [] meronyms: []\n",
            "-----------------------------------\n",
            "token:\n",
            "a and lemma is a\n",
            "-------\n",
            "SYSNSET:Synset('angstrom.n.01')\n",
            "hypernyms: [Synset('metric_linear_unit.n.01')] hyponyms: [] holonyms: [] meronyms: [Synset('picometer.n.01')]\n",
            "SYSNSET:Synset('vitamin_a.n.01')\n",
            "hypernyms: [Synset('fat-soluble_vitamin.n.01')] hyponyms: [Synset('vitamin_a1.n.01'), Synset('vitamin_a2.n.01')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('deoxyadenosine_monophosphate.n.01')\n",
            "hypernyms: [Synset('nucleotide.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('adenine.n.01')\n",
            "hypernyms: [Synset('purine.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('ampere.n.02')\n",
            "hypernyms: [Synset('current_unit.n.01')] hyponyms: [] holonyms: [] meronyms: [Synset('milliampere.n.01')]\n",
            "SYSNSET:Synset('a.n.06')\n",
            "hypernyms: [Synset('letter.n.02')] hyponyms: [] holonyms: [Synset('roman_alphabet.n.01')] meronyms: []\n",
            "SYSNSET:Synset('a.n.07')\n",
            "hypernyms: [Synset('blood_group.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "-----------------------------------\n",
            "token:\n",
            "lie and lemma is lie\n",
            "-------\n",
            "SYSNSET:Synset('lie.n.01')\n",
            "hypernyms: [Synset('falsehood.n.01')] hyponyms: [Synset('fib.n.01'), Synset('jactitation.n.02'), Synset('white_lie.n.01'), Synset('whopper.n.01')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('lie.n.02')\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('lie.n.03')\n",
            "hypernyms: [Synset('position.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('lie.v.01')\n",
            "hypernyms: [Synset('be.v.03')] hyponyms: [Synset('back.v.06'), Synset('cap.v.01'), Synset('dominate.v.05'), Synset('flank.v.01'), Synset('front.v.01'), Synset('head.v.07'), Synset('intervene.v.02'), Synset('lap.v.01'), Synset('line.v.01'), Synset('localize.v.02'), Synset('look_out_on.v.01'), Synset('mediate.v.02'), Synset('nestle.v.02'), Synset('orient.v.01'), Synset('precede.v.02'), Synset('ride.v.09'), Synset('slant.v.01'), Synset('top.v.03'), Synset('underlie.v.02')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('lie.v.02')\n",
            "hypernyms: [] hyponyms: [Synset('bask.v.02'), Synset('lie_awake.v.01'), Synset('overlie.v.01'), Synset('recumb.v.01'), Synset('repose.v.03'), Synset('sprawl.v.01'), Synset('sun.v.01')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('dwell.v.02')\n",
            "hypernyms: [Synset('exist.v.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('lie.v.04')\n",
            "hypernyms: [Synset('be.v.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('lie.v.05')\n",
            "hypernyms: [Synset('misinform.v.01')] hyponyms: [Synset('fib.v.01'), Synset('perjure.v.01'), Synset('romance.v.04')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('lie.v.06')\n",
            "hypernyms: [Synset('be.v.03')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('lie_down.v.01')\n",
            "hypernyms: [Synset('change_posture.v.01')] hyponyms: [Synset('charge.v.13'), Synset('prostrate.v.01'), Synset('stretch.v.06')] holonyms: [] meronyms: []\n",
            "-----------------------------------\n",
            "token:\n",
            "for and lemma is for\n",
            "-------\n",
            "-----------------------------------\n",
            "token:\n",
            "several and lemma is several\n",
            "-------\n",
            "SYSNSET:Synset('several.s.01')\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('respective.s.01')\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('several.s.03')\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "-----------------------------------\n",
            "token:\n",
            "years and lemma is year\n",
            "-------\n",
            "SYSNSET:Synset('year.n.01')\n",
            "hypernyms: [Synset('time_period.n.01')] hyponyms: [Synset('annum.n.01'), Synset('calendar_year.n.01'), Synset('church_year.n.01'), Synset('common_year.n.01'), Synset('fiscal_year.n.01'), Synset('holy_year.n.01'), Synset('leap_year.n.01'), Synset('new_year.n.01'), Synset('off_year.n.01'), Synset('off_year.n.02'), Synset('y2k.n.01'), Synset('year_of_grace.n.01')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('year.n.02')\n",
            "hypernyms: [Synset('time_period.n.01')] hyponyms: [Synset('school_year.n.01')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('year.n.03')\n",
            "hypernyms: [Synset('time_period.n.01')] hyponyms: [Synset('anomalistic_year.n.01'), Synset('lunar_year.n.01'), Synset('sidereal_year.n.01'), Synset('solar_year.n.01')] holonyms: [] meronyms: [Synset('month.n.02'), Synset('season.n.02')]\n",
            "SYSNSET:Synset('class.n.06')\n",
            "hypernyms: [Synset('gathering.n.01')] hyponyms: [Synset('freshman_class.n.01'), Synset('graduating_class.n.01'), Synset('junior_class.n.01'), Synset('senior_class.n.01'), Synset('sophomore_class.n.01')] holonyms: [] meronyms: []\n",
            "-----------------------------------\n",
            "token:\n",
            "by and lemma is by\n",
            "-------\n",
            "SYSNSET:Synset('by.r.01')\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('aside.r.06')\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "-----------------------------------\n",
            "token:\n",
            "their and lemma is -PRON-\n",
            "-------\n",
            "-----------------------------------\n",
            "token:\n",
            "parents and lemma is parent\n",
            "-------\n",
            "SYSNSET:Synset('parent.n.01')\n",
            "hypernyms: [Synset('genitor.n.01')] hyponyms: [Synset('adoptive_parent.n.01'), Synset('empty_nester.n.01'), Synset('father.n.01'), Synset('filicide.n.01'), Synset('mother.n.01'), Synset('stepparent.n.01')] holonyms: [Synset('family.n.02')] meronyms: []\n",
            "SYSNSET:Synset('parent.n.02')\n",
            "hypernyms: [Synset('organism.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('rear.v.02')\n",
            "hypernyms: [] hyponyms: [Synset('cradle.v.02'), Synset('fledge.v.01'), Synset('foster.v.02')] holonyms: [] meronyms: []\n",
            "-----------------------------------\n",
            "token:\n",
            "before and lemma is before\n",
            "-------\n",
            "SYSNSET:Synset('earlier.r.01')\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('ahead.r.01')\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "-----------------------------------\n",
            "token:\n",
            "he and lemma is -PRON-\n",
            "-------\n",
            "-----------------------------------\n",
            "token:\n",
            "/ and lemma is /\n",
            "-------\n",
            "-----------------------------------\n",
            "token:\n",
            "she and lemma is -PRON-\n",
            "-------\n",
            "-----------------------------------\n",
            "token:\n",
            "realizes and lemma is realize\n",
            "-------\n",
            "SYSNSET:Synset('recognize.v.02')\n",
            "hypernyms: [Synset('know.v.01')] hyponyms: [Synset('know.v.09')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('understand.v.02')\n",
            "hypernyms: [] hyponyms: [Synset('appreciate.v.02'), Synset('perceive.v.02')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('realize.v.03')\n",
            "hypernyms: [Synset('make.v.03')] hyponyms: [Synset('express.v.05'), Synset('incarnate.v.01')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('gain.v.08')\n",
            "hypernyms: [Synset('get.v.01')] hyponyms: [Synset('eke_out.v.03'), Synset('gross.v.01'), Synset('profit.v.02'), Synset('rake_in.v.01'), Synset('rake_off.v.01'), Synset('take_home.v.01'), Synset('yield.v.10')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('realize.v.05')\n",
            "hypernyms: [Synset('sell.v.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('realize.v.06')\n",
            "hypernyms: [Synset('harmonize.v.02')] hyponyms: [] holonyms: [] meronyms: []\n",
            "-----------------------------------\n",
            "token:\n",
            "that and lemma is that\n",
            "-------\n",
            "-----------------------------------\n",
            "token:\n",
            "a and lemma is a\n",
            "-------\n",
            "SYSNSET:Synset('angstrom.n.01')\n",
            "hypernyms: [Synset('metric_linear_unit.n.01')] hyponyms: [] holonyms: [] meronyms: [Synset('picometer.n.01')]\n",
            "SYSNSET:Synset('vitamin_a.n.01')\n",
            "hypernyms: [Synset('fat-soluble_vitamin.n.01')] hyponyms: [Synset('vitamin_a1.n.01'), Synset('vitamin_a2.n.01')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('deoxyadenosine_monophosphate.n.01')\n",
            "hypernyms: [Synset('nucleotide.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('adenine.n.01')\n",
            "hypernyms: [Synset('purine.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('ampere.n.02')\n",
            "hypernyms: [Synset('current_unit.n.01')] hyponyms: [] holonyms: [] meronyms: [Synset('milliampere.n.01')]\n",
            "SYSNSET:Synset('a.n.06')\n",
            "hypernyms: [Synset('letter.n.02')] hyponyms: [] holonyms: [Synset('roman_alphabet.n.01')] meronyms: []\n",
            "SYSNSET:Synset('a.n.07')\n",
            "hypernyms: [Synset('blood_group.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "-----------------------------------\n",
            "token:\n",
            "Santa and lemma is Santa\n",
            "-------\n",
            "SYSNSET:Synset('santa_claus.n.01')\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "-----------------------------------\n",
            "token:\n",
            "Claus and lemma is Claus\n",
            "-------\n",
            "-----------------------------------\n",
            "token:\n",
            "does and lemma is do\n",
            "-------\n",
            "SYSNSET:Synset('bash.n.02')\n",
            "hypernyms: [Synset('party.n.04')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('do.n.02')\n",
            "hypernyms: [Synset('solfa_syllable.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('doctor_of_osteopathy.n.01')\n",
            "hypernyms: [Synset('doctor's_degree.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('make.v.01')\n",
            "hypernyms: [] hyponyms: [Synset('overdo.v.01')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('perform.v.01')\n",
            "hypernyms: [] hyponyms: [Synset('appear.v.06'), Synset('blaze_away.v.01'), Synset('carry.v.39'), Synset('churn_out.v.01'), Synset('click_off.v.01'), Synset('conduct.v.02'), Synset('cut.v.23'), Synset('cut_corners.v.01'), Synset('declaim.v.01'), Synset('improvise.v.01'), Synset('interpret.v.03'), Synset('make.v.16'), Synset('pipe_up.v.01'), Synset('premier.v.02'), Synset('rehearse.v.01'), Synset('scamp.v.01'), Synset('serenade.v.01'), Synset('star.v.02'), Synset('stunt.v.02')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('do.v.03')\n",
            "hypernyms: [Synset('carry_through.v.01')] hyponyms: [Synset('go_all_out.v.01'), Synset('misdo.v.01'), Synset('overachieve.v.01'), Synset('ply.v.02'), Synset('turn.v.17'), Synset('underachieve.v.01')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('do.v.04')\n",
            "hypernyms: [Synset('proceed.v.04')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('cause.v.01')\n",
            "hypernyms: [Synset('make.v.03')] hyponyms: [Synset('determine.v.02'), Synset('effect.v.01'), Synset('engender.v.01'), Synset('facilitate.v.03'), Synset('impel.v.01'), Synset('initiate.v.02'), Synset('make.v.08'), Synset('motivate.v.01'), Synset('occasion.v.01'), Synset('provoke.v.02')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('practice.v.01')\n",
            "hypernyms: [] hyponyms: [Synset('shamanize.v.01')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('suffice.v.01')\n",
            "hypernyms: [Synset('satisfy.v.01')] hyponyms: [Synset('go_a_long_way.v.01'), Synset('go_around.v.01'), Synset('qualify.v.01'), Synset('serve.v.01'), Synset('tide_over.v.01')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('do.v.08')\n",
            "hypernyms: [Synset('create.v.05')] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('act.v.02')\n",
            "hypernyms: [] hyponyms: [Synset('act.v.05'), Synset('act_involuntarily.v.01'), Synset('backslap.v.01'), Synset('break_down.v.03'), Synset('bungle.v.02'), Synset('dally.v.02'), Synset('fall_over_backwards.v.01'), Synset('follow.v.18'), Synset('footle.v.02'), Synset('freeze.v.10'), Synset('frivol.v.01'), Synset('hugger_mugger.v.01'), Synset('joke.v.02'), Synset('make.v.19'), Synset('make.v.48'), Synset('make_as_if.v.01'), Synset('menace.v.03'), Synset('optimize.v.03'), Synset('piffle.v.02'), Synset('play.v.16'), Synset('presume.v.04'), Synset('quack.v.02'), Synset('ramp.v.01'), Synset('relax.v.05'), Synset('romanticize.v.03'), Synset('sauce.v.01'), Synset('sentimentalise.v.03'), Synset('stooge.v.03'), Synset('swagger.v.03'), Synset('swell.v.02'), Synset('vulgarize.v.03'), Synset('wanton.v.06')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('serve.v.09')\n",
            "hypernyms: [Synset('spend.v.01')] hyponyms: [Synset('admit.v.08')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('do.v.11')\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('dress.v.16')\n",
            "hypernyms: [Synset('groom.v.03')] hyponyms: [Synset('bob.v.05'), Synset('wave.v.05')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('do.v.13')\n",
            "hypernyms: [Synset('travel.v.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "-----------------------------------\n",
            "token:\n",
            "not and lemma is not\n",
            "-------\n",
            "SYSNSET:Synset('not.r.01')\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "-----------------------------------\n",
            "token:\n",
            "exist and lemma is exist\n",
            "-------\n",
            "SYSNSET:Synset('exist.v.01')\n",
            "hypernyms: [] hyponyms: [Synset('coexist.v.02'), Synset('come.v.06'), Synset('distribute.v.08'), Synset('dwell.v.02'), Synset('dwell.v.04'), Synset('endanger.v.01'), Synset('flow.v.04'), Synset('indwell.v.01'), Synset('kick_around.v.01'), Synset('preexist.v.01'), Synset('prevail.v.02')] holonyms: [] meronyms: []\n",
            "SYSNSET:Synset('exist.v.02')\n",
            "hypernyms: [] hyponyms: [Synset('breathe.v.02'), Synset('freewheel.v.01')] holonyms: [] meronyms: []\n",
            "-----------------------------------\n",
            "token:\n",
            ". and lemma is .\n",
            "-------\n",
            "-----------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"d44021e203c748339993dbce8c3a671d-0\" class=\"displacy\" width=\"4075\" height=\"662.0\" direction=\"ltr\" style=\"max-width: none; height: 662.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">A</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">child</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">told</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">lie</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">for</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">several</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">years</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">by</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">their</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">parents</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">before</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">he/</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">she</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">realizes</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">that</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">SCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">Santa</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3375\">Claus</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3375\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3550\">does</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3550\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3725\">not</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3725\">PART</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3900\">exist.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3900\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d44021e203c748339993dbce8c3a671d-0-0\" stroke-width=\"2px\" d=\"M70,527.0 C70,439.5 200.0,439.5 200.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d44021e203c748339993dbce8c3a671d-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,529.0 L62,517.0 78,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d44021e203c748339993dbce8c3a671d-0-1\" stroke-width=\"2px\" d=\"M245,527.0 C245,352.0 555.0,352.0 555.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d44021e203c748339993dbce8c3a671d-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubjpass</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,529.0 L237,517.0 253,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d44021e203c748339993dbce8c3a671d-0-2\" stroke-width=\"2px\" d=\"M420,527.0 C420,439.5 550.0,439.5 550.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d44021e203c748339993dbce8c3a671d-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">auxpass</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,529.0 L412,517.0 428,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d44021e203c748339993dbce8c3a671d-0-3\" stroke-width=\"2px\" d=\"M770,527.0 C770,439.5 900.0,439.5 900.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d44021e203c748339993dbce8c3a671d-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M770,529.0 L762,517.0 778,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d44021e203c748339993dbce8c3a671d-0-4\" stroke-width=\"2px\" d=\"M595,527.0 C595,352.0 905.0,352.0 905.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d44021e203c748339993dbce8c3a671d-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M905.0,529.0 L913.0,517.0 897.0,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d44021e203c748339993dbce8c3a671d-0-5\" stroke-width=\"2px\" d=\"M595,527.0 C595,264.5 1085.0,264.5 1085.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d44021e203c748339993dbce8c3a671d-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1085.0,529.0 L1093.0,517.0 1077.0,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d44021e203c748339993dbce8c3a671d-0-6\" stroke-width=\"2px\" d=\"M1295,527.0 C1295,439.5 1425.0,439.5 1425.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d44021e203c748339993dbce8c3a671d-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1295,529.0 L1287,517.0 1303,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d44021e203c748339993dbce8c3a671d-0-7\" stroke-width=\"2px\" d=\"M1120,527.0 C1120,352.0 1430.0,352.0 1430.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d44021e203c748339993dbce8c3a671d-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1430.0,529.0 L1438.0,517.0 1422.0,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d44021e203c748339993dbce8c3a671d-0-8\" stroke-width=\"2px\" d=\"M595,527.0 C595,177.0 1615.0,177.0 1615.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d44021e203c748339993dbce8c3a671d-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">agent</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1615.0,529.0 L1623.0,517.0 1607.0,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d44021e203c748339993dbce8c3a671d-0-9\" stroke-width=\"2px\" d=\"M1820,527.0 C1820,439.5 1950.0,439.5 1950.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d44021e203c748339993dbce8c3a671d-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1820,529.0 L1812,517.0 1828,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d44021e203c748339993dbce8c3a671d-0-10\" stroke-width=\"2px\" d=\"M1645,527.0 C1645,352.0 1955.0,352.0 1955.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d44021e203c748339993dbce8c3a671d-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1955.0,529.0 L1963.0,517.0 1947.0,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d44021e203c748339993dbce8c3a671d-0-11\" stroke-width=\"2px\" d=\"M595,527.0 C595,2.0 2150.0,2.0 2150.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d44021e203c748339993dbce8c3a671d-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2150.0,529.0 L2158.0,517.0 2142.0,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d44021e203c748339993dbce8c3a671d-0-12\" stroke-width=\"2px\" d=\"M2170,527.0 C2170,439.5 2300.0,439.5 2300.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d44021e203c748339993dbce8c3a671d-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2300.0,529.0 L2308.0,517.0 2292.0,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d44021e203c748339993dbce8c3a671d-0-13\" stroke-width=\"2px\" d=\"M2520,527.0 C2520,439.5 2650.0,439.5 2650.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d44021e203c748339993dbce8c3a671d-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2520,529.0 L2512,517.0 2528,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d44021e203c748339993dbce8c3a671d-0-14\" stroke-width=\"2px\" d=\"M2870,527.0 C2870,177.0 3890.0,177.0 3890.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d44021e203c748339993dbce8c3a671d-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2870,529.0 L2862,517.0 2878,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d44021e203c748339993dbce8c3a671d-0-15\" stroke-width=\"2px\" d=\"M3045,527.0 C3045,352.0 3355.0,352.0 3355.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d44021e203c748339993dbce8c3a671d-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3045,529.0 L3037,517.0 3053,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d44021e203c748339993dbce8c3a671d-0-16\" stroke-width=\"2px\" d=\"M3220,527.0 C3220,439.5 3350.0,439.5 3350.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d44021e203c748339993dbce8c3a671d-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3220,529.0 L3212,517.0 3228,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d44021e203c748339993dbce8c3a671d-0-17\" stroke-width=\"2px\" d=\"M3395,527.0 C3395,264.5 3885.0,264.5 3885.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d44021e203c748339993dbce8c3a671d-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3395,529.0 L3387,517.0 3403,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d44021e203c748339993dbce8c3a671d-0-18\" stroke-width=\"2px\" d=\"M3570,527.0 C3570,352.0 3880.0,352.0 3880.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d44021e203c748339993dbce8c3a671d-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3570,529.0 L3562,517.0 3578,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d44021e203c748339993dbce8c3a671d-0-19\" stroke-width=\"2px\" d=\"M3745,527.0 C3745,439.5 3875.0,439.5 3875.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d44021e203c748339993dbce8c3a671d-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">neg</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3745,529.0 L3737,517.0 3753,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d44021e203c748339993dbce8c3a671d-0-20\" stroke-width=\"2px\" d=\"M2695,527.0 C2695,89.5 3895.0,89.5 3895.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d44021e203c748339993dbce8c3a671d-0-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3895.0,529.0 L3903.0,517.0 3887.0,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "***************************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWmoGnOW4AgO",
        "outputId": "e225cad3-9c63-44e2-9ec2-96e8e0051197"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from spacy import displacy"
      ],
      "execution_count": 326,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdrEQs4W4UiF"
      },
      "source": [
        "# Extracting the part of speech tags of the entities\n",
        "\n",
        "itr = 0\n",
        "e1_pos = []\n",
        "e2_pos = []\n",
        "\n",
        "from itertools import groupby\n",
        "\n",
        "def all_equal(iterable):\n",
        "    g = groupby(iterable)\n",
        "    return next(g, True) and not next(g, False)\n",
        "\n",
        "for p in sentences:\n",
        "  pos_list1 = []\n",
        "  pos_list2 = []\n",
        "  entity1_words = []\n",
        "  entity1_word = entity1[itr]\n",
        "  entity2_words = []\n",
        "  entity2_word = entity2[itr]\n",
        "  token2doc = nltk.word_tokenize(p)\n",
        "  token3doc = nltk.pos_tag(token2doc)\n",
        "\n",
        "  for token in token3doc:\n",
        "    if(token[0] in entity1_word):\n",
        "      pos_list1.append(token[1])\n",
        "    if(token[0] in entity2_word):\n",
        "      pos_list2.append(token[1])\n",
        "      \n",
        "  if((entity1[itr])):\n",
        "      e1_pos.append(pos_list1)\n",
        "  else:\n",
        "    e1_pos.append('')\n",
        "  if((entity2[itr])):\n",
        "      e2_pos.append(pos_list2)\n",
        "  else:\n",
        "    e2_pos.append('')\n",
        "  itr = itr + 1"
      ],
      "execution_count": 327,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzW39lZ_4f4V"
      },
      "source": [
        "# Extracting the dep tags of the entities\n",
        "\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "\n",
        "itr2 = 0\n",
        "e1_dep = []\n",
        "e2_dep = []\n",
        "\n",
        "from itertools import groupby\n",
        "\n",
        "def all_equal(iterable):\n",
        "    g = groupby(iterable)\n",
        "    return next(g, True) and not next(g, False)\n",
        "\n",
        "for p in sentences:\n",
        "  dep_list1 = []\n",
        "  dep_list2 = []\n",
        "  entity1_words = []\n",
        "  entity1_words = entity1[itr2].replace('-', ' ').split()\n",
        "  entity2_words = []\n",
        "  entity2_words = entity2[itr2].replace('-', ' ').split()\n",
        "\n",
        "  for token in nlp(p):\n",
        "    if(token.text in entity1_words):\n",
        "      dep_list1.append(token.dep_)\n",
        "    if(token.text in entity2_words):\n",
        "      dep_list2.append(token.dep_)\n",
        "\n",
        "  if((entity1[itr2])):\n",
        "    if(all_equal(dep_list1)):\n",
        "      e1_dep.append(dep_list1[0])\n",
        "    else:\n",
        "      e1_dep.append(dep_list1[-1])\n",
        "  else:\n",
        "    e1_dep.append('')\n",
        "  if((entity2[itr2])):\n",
        "    if(all_equal(dep_list2)):\n",
        "      e2_dep.append(dep_list2[0])\n",
        "    else:\n",
        "      e2_dep.append(dep_list2[-1])\n",
        "  else:\n",
        "    e2_dep.append('')\n",
        "  itr2 = itr2 + 1"
      ],
      "execution_count": 328,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMs7xIvo4qfy"
      },
      "source": [
        "rel_wo_dir = []\n",
        "for r in rel:\n",
        "  a = r.split('(')[0]\n",
        "  if(r.find('Other') == -1):\n",
        "    rel_wo_dir.append(a)\n",
        "  else:\n",
        "    a = 'Other'\n",
        "    rel_wo_dir.append(a)\n",
        "\n",
        "# print(len(rel_wo_dir))\n",
        "# print(len(entity1))\n",
        "# print(len(entity2))\n",
        "# print(len(e1_pos))\n",
        "# print(len(e2_pos))\n",
        "# print(len(e1_dep))\n",
        "# print(len(e2_dep))\n",
        "# print(len(in_between_list))"
      ],
      "execution_count": 329,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiWT7_Ey4u-M"
      },
      "source": [
        "# Task 3 of the project:\n",
        "\n",
        "matches_list = []\n",
        "relation_direction = []\n",
        "for var in in_between_list:\n",
        "  matches_list.append('')\n",
        "  relation_direction.append('')"
      ],
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-QdbyJt43qH"
      },
      "source": [
        "# Message-Topic Component\n",
        "from spacy.matcher import Matcher\n",
        "unresolved_matcher = Matcher(nlp.vocab)\n",
        "\n",
        "#defining rules to detect Message-Topic relation examples\n",
        "#handling message-topic patterns:\n",
        "\n",
        "v = 0\n",
        "# unresolved_list = ['Instrument-Agency','Message-Topic']\n",
        "unresolved_pattern1 = [{'POS': 'VERB'}]\n",
        "unresolved_pattern2 = [{'LEMMA': 'topic'}]\n",
        "                      \n",
        "unresolved_matcher.add('UNRESOLVED_MATCHER', None, unresolved_pattern1)\n",
        "unresolved_matcher.add('UNRESOLVED_MATCHER', None, unresolved_pattern2)\n",
        "\n",
        "for var in sentences:\n",
        "  entity1_words = []\n",
        "  entity1_words = entity1[v].split()\n",
        "  entity2_words = []\n",
        "  entity2_words = entity2[v].split()\n",
        "  e1_flag = 0\n",
        "  e2_flag = 0\n",
        "  verb_flag = 0\n",
        "\n",
        "  for token in nlp(var):\n",
        "    if(token.text in entity1_words):\n",
        "      if(token.pos_ == 'NOUN'):\n",
        "        e1_flag = 1\n",
        "    if(token.text in entity2_words):\n",
        "      if(token.pos_ == 'NOUN'):\n",
        "        e2_flag = 1\n",
        "  unresolved_doc = nlp(in_between_list[v])\n",
        "  matches = unresolved_matcher(unresolved_doc)\n",
        "  if(len(matches)>=1):\n",
        "    verb_flag = 1\n",
        "  if((e1_flag == 1) and (e2_flag == 1) and (verb_flag == 1)):\n",
        "    matches_list[v] = 'Message-Topic'\n",
        "    if(e1_dep[v] == 'nsubjpass' and e2_dep[v] == 'pobj'):\n",
        "      relation_direction[v] = 'Message-Topic(e2,e1)'\n",
        "    elif(e1_dep[v] == 'nsubjpass' and e2_dep[v] == 'conj'):\n",
        "      relation_direction[v] = 'Message-Topic(e2,e1)'\n",
        "    else:\n",
        "      relation_direction[v] = 'Message-Topic(e1,e2)'\n",
        "  v = v+1"
      ],
      "execution_count": 331,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43FaJ5LD5LaA"
      },
      "source": [
        "# Instrument-Agency Component\n",
        "from spacy.matcher import Matcher\n",
        "ia_matcher = Matcher(nlp.vocab)\n",
        "\n",
        "#defining rules to detect Instrument-Agency relation examples\n",
        "#handling Instrument-Agency patterns:\n",
        "\n",
        "v = 0\n",
        "ia_pattern1 = [{'LEMMA': 'with'}, {'POS': 'DET'}]\n",
        "ia_pattern2 = [{'LEMMA': 'by'}, {'LEMMA': 'using'}]\n",
        "ia_pattern3 = [{'LEMMA': 'use'}, {'POS': 'DET'}]\n",
        "ia_pattern4 = [{'LEMMA': 'using'}, {'POS': 'DET'}]\n",
        "# ia_pattern5 = [{'POS': 'ADP'}]\n",
        "\n",
        "ia_matcher.add('INSTRUMENT_AGENCY_PATTERN', None, ia_pattern1)\n",
        "ia_matcher.add('INSTRUMENT_AGENCY_PATTERN', None, ia_pattern2)\n",
        "ia_matcher.add('INSTRUMENT_AGENCY_PATTERN', None, ia_pattern3)\n",
        "ia_matcher.add('INSTRUMENT_AGENCY_PATTERN', None, ia_pattern4)\n",
        "# ia_matcher.add('INSTRUMENT_AGENCY_PATTERN', None, ia_pattern5)\n",
        "\n",
        "for var in sentences:\n",
        "  entity1_words = []\n",
        "  entity1_words = entity1[v].split()\n",
        "  entity2_words = []\n",
        "  entity2_words = entity2[v].split()\n",
        "  e1_flag = 0\n",
        "  e2_flag = 0\n",
        "  verb_flag = 0\n",
        "\n",
        "  for token in nlp(var):\n",
        "    if(token.text in entity1_words):\n",
        "      if(token.pos_ == 'NOUN'):\n",
        "        e1_flag = 1\n",
        "    if(token.text in entity2_words):\n",
        "      if(token.pos_ == 'NOUN'):\n",
        "        e2_flag = 1\n",
        "  ia_doc = nlp(in_between_list[v])\n",
        "  matches = ia_matcher(ia_doc)\n",
        "  if(len(matches)>=1):\n",
        "    verb_flag = 1\n",
        "  if((e1_flag == 1) and (e2_flag == 1) and (verb_flag == 1)):\n",
        "    matches_list[v] = 'Instrument-Agency'\n",
        "    if(e1_dep[v] == 'nsubj'):\n",
        "      if(e2_dep[v] == 'pobj' or e2_dep[v] == 'dobj'):\n",
        "        relation_direction[v] = 'Instrument-Agency(e2,e1)'\n",
        "      else:\n",
        "        relation_direction[v] = 'Instrument-Agency(e1,e2)'\n",
        "    elif(e1_dep[v] == 'compound'):\n",
        "      if(e2_dep[v] == 'pobj'):\n",
        "        relation_direction[v] = 'Instrument-Agency(e2,e1)'\n",
        "      else:\n",
        "        relation_direction[v] = 'Instrument-Agency(e1,e2)'\n",
        "    else:\n",
        "      relation_direction[v] = 'Instrument-Agency(e1,e2)'\n",
        "  v = v+1"
      ],
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm-V3hh35fAa"
      },
      "source": [
        "# Product-Producer Component\n",
        "\n",
        "#defining rules to detect Product-Producer relation examples\n",
        "#handling Product-Producer patterns:\n",
        "itr = 0\n",
        "\n",
        "product_producer_set = set()\n",
        "product_producer_set.add('made')\n",
        "product_producer_set.add('discovered')\n",
        "product_producer_set.add('discover')\n",
        "product_producer_set.add('found')\n",
        "product_producer_set.add('created')\n",
        "product_producer_set.add('create')\n",
        "product_producer_set.add('invented')\n",
        "product_producer_set.add('invent')\n",
        "product_producer_set.add('developed')\n",
        "product_producer_set.add('develop')\n",
        "product_producer_set.add('order')\n",
        "product_producer_set.add('manufactured')\n",
        "product_producer_set.add('make')\n",
        "\n",
        "for var in sentences:\n",
        "  wordnet_set=set()\n",
        "  doc = nlp(var)\n",
        "  for token in doc:\n",
        "    if(token.pos_ == 'VERB'):\n",
        "      syn_list=wn.synsets(token.text)\n",
        "      syn_list=[word for word in syn_list if word.name().split('.')[1]=='v']\n",
        "      for i in range(0,len(syn_list)):\n",
        "        for x in (val for val in syn_list[i].hypernyms()):\n",
        "          x_val = x._name.split('.')[0]\n",
        "          wordnet_set.add(x_val)\n",
        "        for x in (val for val in syn_list[i].root_hypernyms()):\n",
        "          x_val = x._name.split('.')[0]\n",
        "          wordnet_set.add(x_val)\n",
        "  # print(i)\n",
        "  # print(wordnet_set)\n",
        "  if(len(product_producer_set.difference(wordnet_set)) <= 12):\n",
        "    matches_list[itr] = 'Product-Producer'\n",
        "    if(e1_dep[itr] == 'nsubjpass'):\n",
        "      if(e2_dep[itr] == 'pobj'):\n",
        "        relation_direction[itr] = 'Product-Producer(e1,e2)'\n",
        "      else:\n",
        "        relation_direction[itr] = 'Product-Producer(e2,e1)'\n",
        "    elif(e1_dep[itr] == 'dobj'):\n",
        "      if(e2_dep[itr] == 'pobj' or e2_dep[itr] == 'nsubj'):\n",
        "        relation_direction[itr] = 'Product-Producer(e1,e2)'\n",
        "      else:\n",
        "        relation_direction[itr] = 'Product-Producer(e2,e1)'\n",
        "    elif(e1_dep[itr] == 'compound'):\n",
        "      relation_direction[itr] = 'Product-Producer(e1,e2)'\n",
        "    elif(e1_dep[itr] == 'attr'):\n",
        "      if(e2_dep[itr] == 'pobj'):\n",
        "        relation_direction[itr] = 'Product-Producer(e1,e2)'\n",
        "      else:\n",
        "        relation_direction[itr] = 'Product-Producer(e2,e1)'\n",
        "    elif(e1_dep[itr] == 'nmod'):\n",
        "      if(e2_dep[itr] == 'appos' or e2_dep[itr] == 'conj'):\n",
        "        relation_direction[itr] = 'Product-Producer(e1,e2)'\n",
        "      else:\n",
        "        relation_direction[itr] = 'Product-Producer(e2,e1)'\n",
        "    elif(e1_dep[itr] == 'pobj'):\n",
        "      relation_direction[itr] = 'Product-Producer(e2,e1)'\n",
        "    elif(e1_dep[itr] == 'nsubj'):\n",
        "      relation_direction[itr] = 'Product-Producer(e2,e1)'\n",
        "    else:\n",
        "      relation_direction[itr] = 'Product-Producer(e2,e1)'\n",
        "  itr = itr+1"
      ],
      "execution_count": 333,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5xCXhIS5zSg"
      },
      "source": [
        "# Member-Collection Component\n",
        "\n",
        "from spacy.matcher import Matcher\n",
        "mem_collec_matcher = Matcher(nlp.vocab)\n",
        "\n",
        "#defining rules to detect Member-Collection relation examples\n",
        "#handling Member-Collection patterns:\n",
        "# Member Collection Patterns:\n",
        "\n",
        "mc_pattern1 = [{'LEMMA': 'of'}, {'POS': 'NOUN'}]\n",
        "mc_pattern2 = [{'LEMMA': 'of'}, {'POS': 'PROPN'}]\n",
        "mc_pattern3 = [{'LEMMA': 'collect'}]\n",
        "mc_pattern4 = [{'LEMMA': 'of'}, {'POS': 'ADJ'}, {'POS': 'NOUN'}]\n",
        "mc_pattern5 = [{'LEMMA': 'of'}, {'POS': 'ADJ'}]\n",
        "mc_pattern6 = [{'LEMMA': 'of'}]\n",
        "\n",
        "mem_collec_matcher.add('MEMBER_COLLECTION_PATTERN', None, mc_pattern1)\n",
        "mem_collec_matcher.add('MEMBER_COLLECTION_PATTERN', None, mc_pattern2)\n",
        "mem_collec_matcher.add('MEMBER_COLLECTION_PATTERN', None, mc_pattern3)\n",
        "mem_collec_matcher.add('MEMBER_COLLECTION_PATTERN', None, mc_pattern4)\n",
        "mem_collec_matcher.add('MEMBER_COLLECTION_PATTERN', None, mc_pattern5)\n",
        "mem_collec_matcher.add('MEMBER_COLLECTION_PATTERN', None, mc_pattern6)\n",
        "\n",
        "\n",
        "v = 0\n",
        "e1_dep_set = set()\n",
        "e1_dep_set.add('nsubj')\n",
        "e1_dep_set.add('attr')\n",
        "e1_dep_set.add('pobj')\n",
        "e1_dep_set.add('nsubjpass')\n",
        "e1_dep_set.add('dobj')\n",
        "e1_dep_set.add('conj')\n",
        "\n",
        "for var in in_between_list:\n",
        "  mc_doc = nlp(var)\n",
        "  matches = mem_collec_matcher(mc_doc)\n",
        "  if(len(matches)>=1):\n",
        "    matches_list[v] = 'Member-Collection'\n",
        "    if((e2_dep == 'pobj') and (e1_dep in e1_dep_set)):\n",
        "      relation_direction[v] = 'Member-Collection(e2,e1)'\n",
        "    elif(e1_dep == 'nsubjpass'):\n",
        "      relation_direction[v] = 'Member-Collection(e1,e2)'\n",
        "    else:\n",
        "      relation_direction[v] = 'Member-Collection(e2,e1)'\n",
        "  v = v + 1"
      ],
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbBSe0jJ6Jfb"
      },
      "source": [
        "# Component-Whole Component\n",
        "from spacy.matcher import Matcher\n",
        "comp_whole_matcher = Matcher(nlp.vocab)\n",
        "\n",
        "\n",
        "#defining rules to detect Component-Whole relation examples\n",
        "#handling Component-Whole patterns:\n",
        "# Component Whole Patterns:\n",
        "cw_pattern1 = [{'LEMMA': 'of'}, {'POS': 'DET'}]\n",
        "cw_pattern2 = [{'LEMMA': 'in'}, {'POS': 'DET'}]\n",
        "cw_pattern3 = [{'POS': 'VERB'}, {'LEMMA': 'into'}]\n",
        "cw_pattern4 = [{'LEMMA': 'inside'}, {'POS': 'DET'}]\n",
        "cw_pattern5 = [{'LEMMA': 'comprise'}]\n",
        "cw_pattern6 = [{'LEMMA': 'of'},{'LEMMA':'the'}]\n",
        "# cw_pattern4 = [{'LEMMA': 'of'}]\n",
        "\n",
        "comp_whole_matcher.add('COMPONENT_WHOLE_PATTERN', None, cw_pattern1)\n",
        "comp_whole_matcher.add('COMPONENT_WHOLE_PATTERN', None, cw_pattern2)\n",
        "comp_whole_matcher.add('COMPONENT_WHOLE_PATTERN', None, cw_pattern3)\n",
        "comp_whole_matcher.add('COMPONENT_WHOLE_PATTERN', None, cw_pattern4)\n",
        "comp_whole_matcher.add('COMPONENT_WHOLE_PATTERN', None, cw_pattern5)\n",
        "comp_whole_matcher.add('COMPONENT_WHOLE_PATTERN', None, cw_pattern6)\n",
        "\n",
        "v = 0\n",
        "for var in in_between_list:\n",
        "  cw_doc = nlp(var)\n",
        "  matches = comp_whole_matcher(cw_doc)\n",
        "  if(len(matches)>=1):\n",
        "    matches_list[v] = 'Component-Whole'\n",
        "    if(e2_dep[v] == 'compound'):\n",
        "      relation_direction[v] = 'Component-Whole(e1,e2)'\n",
        "    elif(e1_dep[v] == 'compound'):\n",
        "      relation_direction[v] = 'Component-Whole(e2,e1)'\n",
        "    elif(e1_dep[v] == 'nsubjpass'):\n",
        "      if(e2_dep[v] == 'pobj' or e2_dep[v] == 'nsubj'):\n",
        "        relation_direction[v] = 'Component-Whole(e1,e2)'\n",
        "      else:\n",
        "        relation_direction[v] = 'Component-Whole(e2,e1)'\n",
        "    elif(e1_dep[v] == 'nsubj'):\n",
        "      if(e2_dep[v] == 'pobj'):\n",
        "        relation_direction[v] = 'Component-Whole(e1,e2)'\n",
        "      else:\n",
        "        relation_direction[v] = 'Component-Whole(e2,e1)'\n",
        "    elif(e1_dep[v] == 'dobj'):\n",
        "      relation_direction[v] = 'Component-Whole(e1,e2)'\n",
        "    elif(e1_dep[v] == 'pobj'):\n",
        "      if(e2_dep[v] == 'dobj' or e2_dep[v] == 'conj'):\n",
        "        relation_direction[v] = 'Component-Whole(e2,e1)'\n",
        "      else:\n",
        "        relation_direction[v] = 'Component-Whole(e2,e1)'\n",
        "    else:\n",
        "      relation_direction[v] = 'Component-Whole(e1,e2)'\n",
        "  v = v + 1"
      ],
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYbWNOC06eqY"
      },
      "source": [
        "# Content-Container Component\n",
        "from spacy.matcher import Matcher\n",
        "cont_cont_matcher = Matcher(nlp.vocab)\n",
        "\n",
        "#defining rules to detect Content-Container relation examples\n",
        "#handling Content-Container patterns:\n",
        "# Content-Container Patterns:\n",
        "cc_pattern1 = [{'POS': 'VERB'}, {'LEMMA': 'in'}, {'LEMMA': 'a'}]\n",
        "cc_pattern2 = [{'POS': 'VERB'}, {'LEMMA': 'inside'}, {'POS': 'DET'}]\n",
        "cc_pattern3 = [{'LEMMA': 'in'}, {'LEMMA': 'a'}]\n",
        "cc_pattern4 = [{'LEMMA': 'contain'}]\n",
        "cc_pattern5 = [{'LEMMA': 'content'}]\n",
        "# cc_pattern6 = [{'LEMMA': 'with'}]\n",
        "# cc_pattern5 = [{'POS': 'ADP'}]\n",
        "\n",
        "cont_cont_matcher.add('CONTENT_CONTAINER_PATTERN', None, cc_pattern1)\n",
        "cont_cont_matcher.add('CONTENT_CONTAINER_PATTERN', None, cc_pattern2)\n",
        "cont_cont_matcher.add('CONTENT_CONTAINER_PATTERN', None, cc_pattern3)\n",
        "cont_cont_matcher.add('CONTENT_CONTAINER_PATTERN', None, cc_pattern4)\n",
        "cont_cont_matcher.add('CONTENT_CONTAINER_PATTERN', None, cc_pattern5)\n",
        "# cont_cont_matcher.add('CONTENT_CONTAINER_PATTERN', None, cc_pattern6)\n",
        "\n",
        "v = 0\n",
        "for var in in_between_list:\n",
        "  cc_doc = nlp(var)\n",
        "  matches = cont_cont_matcher(cc_doc)\n",
        "  if(len(matches)>=1):\n",
        "    matches_list[v] = 'Content-Container'\n",
        "    if(e1_dep[v] == 'nsubj' and e1_dep[v] == 'dobj'):\n",
        "      relation_direction[v] = 'Content-Container(e2,e1)'\n",
        "    else:\n",
        "      relation_direction[v] = 'Content-Container(e1,e2)'\n",
        "  v = v + 1"
      ],
      "execution_count": 336,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RObdvLQZ6pHT"
      },
      "source": [
        "# Cause-Effect Component\n",
        "from spacy.matcher import Matcher\n",
        "cause_eff_matcher = Matcher(nlp.vocab)\n",
        "\n",
        "#defining rules to detect Cause-Effect relation examples\n",
        "#handling Cause-Effect patterns:\n",
        "# Cause-Effect Patterns:\n",
        "ce_pattern1 = [{'LEMMA': 'cause'}]\n",
        "ce_pattern2 = [{'LEMMA': 'from'}]\n",
        "ce_pattern3 = [{'LEMMA': 'from'}, {'POS': 'DET'}]\n",
        "ce_pattern4 = [{'POS': 'VERB'}, {'LEMMA': 'by'}]\n",
        "ce_pattern5 = [{'POS': 'VERB'}, {'LEMMA': 'to'}]\n",
        "# ce_pattern6 = [{'POS': 'VERB'}, {'LEMMA': 'in'}]\n",
        "\n",
        "cause_eff_matcher.add('CAUSE_EFFECT_PATTERN', None, ce_pattern1)\n",
        "cause_eff_matcher.add('CAUSE_EFFECT_PATTERN', None, ce_pattern2)\n",
        "cause_eff_matcher.add('CAUSE_EFFECT_PATTERN', None, ce_pattern3)\n",
        "cause_eff_matcher.add('CAUSE_EFFECT_PATTERN', None, ce_pattern4)\n",
        "cause_eff_matcher.add('CAUSE_EFFECT_PATTERN', None, ce_pattern5)\n",
        "# cause_eff_matcher.add('CAUSE_EFFECT_PATTERN', None, ce_pattern6)\n",
        "\n",
        "v = 0\n",
        "for var in in_between_list:\n",
        "  ce_doc = nlp(var)\n",
        "  matches = cause_eff_matcher(ce_doc)\n",
        "  if(len(matches)>=1):\n",
        "    matches_list[v] = 'Cause-Effect'\n",
        "    if(e1_dep[v] == 'compound' or e1_dep[v] == 'nsubjpass' or e1_dep[v] == 'attr' or e2_dep[v] == 'pcomp'):\n",
        "      relation_direction[v] = 'Cause-Effect(e2,e1)'\n",
        "    elif(e1_dep[v] == 'conj'):\n",
        "      if(e2_dep[v] == 'pobj' or e2_dep[v] == 'amod' or e2_dep[v] == 'conj'):\n",
        "        relation_direction[v] = 'Cause-Effect(e2,e1)'\n",
        "      else:\n",
        "        relation_direction[v] = 'Cause-Effect(e1,e2)'\n",
        "    elif(e1_dep[v] == 'pobj'):\n",
        "      if(e2_dep[v] == 'pobj' or e2_dep[v] == 'amod'):\n",
        "        relation_direction[v] = 'Cause-Effect(e2,e1)'\n",
        "      else:\n",
        "        relation_direction[v] = 'Cause-Effect(e1,e2)'\n",
        "    elif(e1_dep[v] == 'dobj'):\n",
        "      if(e2_dep[v] == 'pcomp' or e2_dep[v] == 'pobj'):\n",
        "        relation_direction[v] = 'Cause-Effect(e2,e1)'\n",
        "      else:\n",
        "        relation_direction[v] = 'Cause-Effect(e1,e2)'\n",
        "    elif(e1_dep[v] == 'nsubj'):\n",
        "      if(e2_dep[v] == 'pobj'):\n",
        "        relation_direction[v] = 'Cause-Effect(e2,e1)'\n",
        "      else:\n",
        "        relation_direction[v] = 'Cause-Effect(e1,e2)'\n",
        "    else:\n",
        "      relation_direction[v] = 'Cause-Effect(e1,e2)'\n",
        "  v = v + 1"
      ],
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am6TyOVb6ykg"
      },
      "source": [
        "# Entity-Destination Component\n",
        "from spacy.matcher import Matcher\n",
        "ent_dest_matcher = Matcher(nlp.vocab)\n",
        "\n",
        "#defining rules to detect Entity-Destination relation examples\n",
        "#handling Entity-Destination patterns:\n",
        "# Entity-Destination Patterns:\n",
        "ed_pattern1 = [{'POS': 'VERB'}, {'LEMMA': 'into'}]\n",
        "ed_pattern2 = [{'LEMMA': 'into'}]\n",
        "# ed_pattern3 = [{'POS': 'ADV'}, {'POS': 'inot'}]\n",
        "# ed_pattern4 = [{'POS': 'ADP'}, {'POS': 'DET'}]\n",
        "\n",
        "\n",
        "ent_dest_matcher.add('ENTITY_DESTINATION_PATTERN', None, ed_pattern1)\n",
        "ent_dest_matcher.add('ENTITY_DESTINATION_PATTERN', None, ed_pattern2)\n",
        "# ed_matcher.add('ENTITY_DESTINATION_PATTERN', None, ed_pattern3)\n",
        "\n",
        "v = 0\n",
        "for var in in_between_list:\n",
        "  ed_doc = nlp(var)\n",
        "  matches = ent_dest_matcher(ed_doc)\n",
        "  if(len(matches)>=1):\n",
        "    matches_list[v] = 'Entity-Destination'\n",
        "    if(e1_dep[v] == 'pobj' and e2_dep[v] == 'dobj'):\n",
        "      relation_direction[v] = 'Entity-Destination(e2,e1)'\n",
        "    else:\n",
        "      relation_direction[v] = 'Entity-Destination(e1,e2)'\n",
        "  v = v + 1"
      ],
      "execution_count": 339,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWTIrTlT67Gc"
      },
      "source": [
        "# Entity-Origin Component\n",
        "from spacy.matcher import Matcher\n",
        "ent_orig_matcher = Matcher(nlp.vocab)\n",
        "\n",
        "#defining rules to detect Entity-Origin relation examples\n",
        "#handling Entity-Origin patterns:\n",
        "# Entity-Origin Patterns:\n",
        "pattern2 = [{'POS': 'VERB'}, {'LEMMA': 'from'}]\n",
        "pattern3 = [{'POS': 'ADV'}, {'LEMMA': 'from'}]\n",
        "\n",
        "ent_orig_matcher.add('PATTERN', None, pattern2)\n",
        "ent_orig_matcher.add('PATTERN', None, pattern3)\n",
        "\n",
        "v = 0\n",
        "for var in in_between_list:\n",
        "  doc_eo = nlp(var)\n",
        "  matches = ent_orig_matcher(doc_eo)\n",
        "  if(len(matches)>=1):\n",
        "    matches_list[v] = 'Entity-Origin'\n",
        "    if(e2_dep[v] == 'pobj' or e2_dep[v] == 'compound'):\n",
        "      relation_direction[v] = 'Entity-Origin(e1,e2)'\n",
        "    elif(e2_dep[v] == 'nsubj'):\n",
        "      relation_direction[v] = 'Entity-Origin(e2,e1)'\n",
        "    else:\n",
        "      relation_direction[v] = 'Entity-Origin(e1,e2)'\n",
        "  if(matches_list[v] == ''):\n",
        "    matches_list[v] = 'Other'\n",
        "    relation_direction[v] = 'Other'\n",
        "  v = v + 1"
      ],
      "execution_count": 340,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1t_x0mt_wig",
        "outputId": "4f123e93-42d2-4b70-e3c6-bfb11864c8d7"
      },
      "source": [
        "# Setting 1: (Assuming only the relation is classified correctly)\n",
        "# The actual output of the test sentence:\n",
        "print(\"Setting 1:\")\n",
        "print(\"The actual output of the test sentence:\", rel_wo_dir[0])\n",
        "print(\"The predicted output of the test sentence:\", matches_list[0])"
      ],
      "execution_count": 341,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting 1:\n",
            "The actual output of the test sentence: Product-Producer\n",
            "The predicted output of the test sentence: Product-Producer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bddFVx1AWmG",
        "outputId": "86ee76ff-22e2-4088-87bc-1a61725e2f87"
      },
      "source": [
        "# Setting 2: (Assuming both the relation and direction is classified correctly)\n",
        "# The actual output of the test sentence:\n",
        "print(\"Setting 2:\")\n",
        "print(\"The actual output of the test sentence:\", rel[0])\n",
        "print(\"The predicted output of the test sentence:\", relation_direction[0])"
      ],
      "execution_count": 342,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting 2:\n",
            "The actual output of the test sentence: Product-Producer(e1,e2)\n",
            "The predicted output of the test sentence: Product-Producer(e1,e2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pykj2v17GIc"
      },
      "source": [
        "# Storing the output in a csv file\n",
        "import csv\n",
        "with open('finaloutput_data.csv', 'w', newline='') as csvfile:\n",
        "  fieldnames = ['Sentence', 'Entity1', 'E1_POS', 'E1_DEP', 'Entity2',  'E2_POS', 'E2_DEP','Relationship','Direction', 'predicted_relationship','predicted_rel_with_direction']\n",
        "  writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "  writer.writeheader()\n",
        "\n",
        "  i = 0\n",
        "  for p in sentences:\n",
        "    writer.writerow({'Sentence': i, 'Entity1': entity1[i], 'E1_POS': e1_pos[i], 'E1_DEP': e1_dep[i], 'Entity2': entity2[i], 'E2_POS': e2_pos[i], 'E2_DEP': e2_dep[i], 'Relationship': rel_wo_dir[i], 'Direction': direction[i], 'predicted_relationship': matches_list[i], 'predicted_rel_with_direction':relation_direction[i]})\n",
        "    i = i+1\n"
      ],
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE25l-li7JNU"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmwYkVE87Nhl",
        "outputId": "5ab96c27-e4d5-4211-a4d9-cdb88237d911"
      },
      "source": [
        "target_names = ['Product-Producer', 'Cause-Effect', 'Component-Whole', 'Member-Collection', 'Message-Topic', 'Entity-Origin', 'Content-Container', 'Entity-Destination', 'Other', 'Instrument Agency']\n",
        "print(\"Results for semeval_test data set for relation only:\")\n",
        "print(classification_report(rel_wo_dir, matches_list, target_names=target_names))"
      ],
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results for semeval_test data set for relation only:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "  Product-Producer       0.54      0.78      0.64       328\n",
            "      Cause-Effect       0.42      0.48      0.45       312\n",
            "   Component-Whole       0.59      0.61      0.60       192\n",
            " Member-Collection       0.73      0.66      0.69       292\n",
            "     Message-Topic       0.75      0.45      0.56       258\n",
            "     Entity-Origin       0.46      0.14      0.22       156\n",
            " Content-Container       0.57      0.79      0.66       233\n",
            "Entity-Destination       0.40      0.27      0.32       261\n",
            "             Other       0.17      0.08      0.11       454\n",
            " Instrument Agency       0.21      0.45      0.28       231\n",
            "\n",
            "          accuracy                           0.46      2717\n",
            "         macro avg       0.48      0.47      0.45      2717\n",
            "      weighted avg       0.46      0.46      0.44      2717\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smrC6-NI7SH5",
        "outputId": "0f8216b2-8944-4566-dc2c-ae38a5f07df5"
      },
      "source": [
        "target_names = ['Message-Topic(e1,e2)', 'Product-Producer(e2,e1)', 'Instrument-Agency(e2,e1)', 'Entity-Destination(e1,e2)', 'Cause-Effect(e2,e1)', 'Component-Whole(e1,e2)', 'Product-Producer(e1,e2)', 'Member-Collection(e2,e1)', 'Other', 'Entity-Origin(e1,e2)', 'Content-Container(e1,e2)', 'Entity-Origin(e2,e1)', 'Cause-Effect(e1,e2)', 'Component-Whole(e2,e1)', 'Content-Container(e2,e1)', 'Instrument-Agency(e1,e2)', 'Message-Topic(e2,e1)', 'Member-Collection(e1,e2)', 'Entity-Destination(e2,e1)']\n",
        "# target_names = ['Product-Producer(e1,e2)', 'Product-Producer(e2,e1)', 'Cause-Effect(e1,e2)', 'Cause-Effect(e2,e1)', 'Component-Whole(e1,e2)', 'Component-Whole(e2,e1)', 'Member-Collection(e1,e2)', 'Member-Collection(e2,e1)', 'Message-Topic(e1,e2)', 'Message-Topic(e2,e1)', 'Entity-Origin(e1,e2)', 'Entity-Origin(e2,e1)', 'Content-Container(e1,e2)', 'Content-Container(e2,e1)', 'Entity-Destination(e1,e2)', 'Entity-Destination(e2,e1)', 'Instrument Agency(e1,e2)', 'Instrument Agency(e2,e1)', 'Other']\n",
        "print(\"Results for semeval_test data set for relation and its directionality:\")\n",
        "print(classification_report(rel, relation_direction, target_names=target_names))"
      ],
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results for semeval_test data set for relation and its directionality:\n",
            "                           precision    recall  f1-score   support\n",
            "\n",
            "     Message-Topic(e1,e2)       0.67      0.56      0.61       134\n",
            "  Product-Producer(e2,e1)       0.42      0.77      0.54       194\n",
            " Instrument-Agency(e2,e1)       0.32      0.49      0.39       162\n",
            "Entity-Destination(e1,e2)       0.13      0.10      0.11       150\n",
            "      Cause-Effect(e2,e1)       0.58      0.75      0.65       153\n",
            "   Component-Whole(e1,e2)       0.00      0.00      0.00        39\n",
            "  Product-Producer(e1,e2)       0.73      0.66      0.69       291\n",
            " Member-Collection(e2,e1)       0.00      0.00      0.00         1\n",
            "                    Other       0.75      0.55      0.63       211\n",
            "     Entity-Origin(e1,e2)       0.00      0.00      0.00        47\n",
            " Content-Container(e1,e2)       0.00      0.00      0.00        22\n",
            "     Entity-Origin(e2,e1)       0.51      0.13      0.21       134\n",
            "      Cause-Effect(e1,e2)       0.00      0.00      0.00        32\n",
            "   Component-Whole(e2,e1)       0.56      0.92      0.70       201\n",
            " Content-Container(e2,e1)       0.37      0.27      0.31       210\n",
            " Instrument-Agency(e1,e2)       0.31      0.16      0.21        51\n",
            "     Message-Topic(e2,e1)       0.17      0.08      0.11       454\n",
            " Member-Collection(e1,e2)       0.15      0.21      0.18       108\n",
            "Entity-Destination(e2,e1)       0.20      0.59      0.30       123\n",
            "\n",
            "                 accuracy                           0.42      2717\n",
            "                macro avg       0.31      0.33      0.30      2717\n",
            "             weighted avg       0.40      0.42      0.39      2717\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}